{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Choosing a Value of K"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "This script is designed to help the user decide on k, the number of clusters/Cloud regimes to produce. It will preform clustering for values of k within the range specified by the user editable k_range variable. For each value of k, this script will create a range of plots First it will create a plot showing the correlation coefficents between the CR centers, then a plot showing the correlation between the space/time corelation of the CRs. Furthermore, after clustering for the first value of k is preformed, a correlation matrice will be made between the current and previous value of K. For example if you were testing values of k from 3 to 7, after the clustering for k=3 and 4 is preformed a plot of the correlations between the CRs from k=3 and k=4 will be created. This is useful to discern if increasing k by one has produced a genuinly new CR, or just split a CR that already existed. If all of the CRs for k=4 are \"highly\" correlated with a CR from k=3, then it's unlikely a new CR was created. The user can also set plot_cr_centers and plot_rfo_graphs to true, and plots of the CR centers as well as spacial maps will be shown for each value of k.\n",
    "\n",
    "Choosing a value of k is still unfortunately not an exact science, and is a weak point in the CR workflow. Previous works have layed out specific conditons and correlation thresholds to define when k is large enough. However these thresholds and condtions tend to change over time, and correlation thresholds from one set of data may yeild un-ideal results when applied to a different set of data. However, here are the general steps and conditons that are used as outlined by [Tselioudis et al. 2013](https://journals.ametsoc.org/view/journals/clim/26/19/jcli-d-13-00024.1.xml):\n",
    "\n",
    "1. Does clustering converge with a tolerance of 0.001? (This is automatically checked, and the user need not do anything)\n",
    "2. Is this convergence insensitive to the intial centroids? (This is checked in the following chapter)\n",
    "3. Is dispersion of all the vectors in each cluster minimized?\n",
    "4. Does a distinctly new cluster appear when increasing k by 1?\n",
    "\n",
    "Condition 1 is automatically checked by the code. If it is not met, an error will be raised and you will be prompted the change hyper-parameters of the k-means algorithm. However, when using wasserstein distance this value of tol (which is a measure of the total change in the cluster centers between iterations) will likely be much too small, and may need to be increased by quite a few orders of magnitude depending on the dataset to be able to reach convergence. Condtion 2 will be fully checked in the next chapter: Testing Cluster Robustness / Repeatability of Results. Condition 3 is a tricky one, and we do not really touch it here. It seems \"the dispersion of all the vectors in each cluster (rms of each vector's distance from the centroid)\" would decrease as k inreases and clusters become more compact, not necesarily reaching a minimum at some optimum value of k. Without additonal condtions or details that do not seem to be provided for this test, it is not particularly useful. Condition 4 is checked using the assortment of correlation matrices that this script will create. If two CRs have a high centroid correlation and space-time correlation with each other than it is likely that a CR has split (as oppose to a new one appearing) when increasing k by 1. This can be further checked by the k+1 correlation matrices. Typicaly studies will state specific correlation thresholds for CRs being too highly correlated (typically from 0.6-0.9 for histogram correlations), however these are not consistant across studies and cannot be consistent and continue to provide excellent results. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    " \n",
    "It is important to have an understanding of the k-means clustering algorithm and how it works. Here's a [good starting resource](https://neptune.ai/blog/k-means-clustering) and [the documentation of sklearns k-means algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) for further documentation of all the variables. Also helpful is familarity with Xarray and NetCDF file formats.\n",
    "\n",
    "- **Time to learn**: 30 minutes\n",
    "\n",
    "- **System requirements**:\n",
    "    - If you wish to use Wasserstein distance the [Wasserstein](https://foundations.projectpythia.org/core/data-formats/netcdf-cf.html) package must be installed (and its ideal to have many processors at your disposal)\n",
    "    - If you wish to run euclidean k-means on a gpu then the system must have an Nvidia GPU"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from Functions import emd_means, euclidean_kmeans, plot_hists, plot_rfo, histogram_cor, spatial_cor, open_and_process, kp1_histogram_cor\n",
    "import logging as lgr\n",
    "import dask"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we define the variables necesary to begin using the toy ISCCP dataset included with this cookbook. To start off leave these variables alone, but later on feel free to experiment. For example if you preform this analysis for only over land does the final value of k you decide on change? Is it smaller or larger? \n",
    "\n",
    "If running locally with your own dataset, you will need to change these to match your data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Path to data to cluster\n",
    "data_path = \"./ISCCP_toy_data.nc\"\n",
    "\n",
    "# Path to the directory you wish to save plots in if running as a script, if None plots will only be shown and not saved. Enter as String\n",
    "save_path = None\n",
    "save_prefix = None  # prefix to put into the name of each plot to distinguish different runs of this script\n",
    "\n",
    "# Variable name of data to cluster in data_path\n",
    "# Name of tau dimension for var_name\n",
    "# Name of height/pressure dimension for var_name\n",
    "var_name =  'n_pctaudist' \n",
    "tau_var_name =  'levtau' \n",
    "ht_var_name =  'levpc'\n",
    "lat_var_name = 'lat'\n",
    "lon_var_name = 'lon'\n",
    "\n",
    "# Does this dataset use cloud top height or cloud top pressure? enter \"h\" for height or \"p\" for pressure\n",
    "height_or_pressure = 'p'\n",
    "\n",
    "# kmeans properties\n",
    "tol = 0.001   # maximum change in inertia values between kmeans iterations to declare convergence. should be higher if using wasserstein distance\n",
    "max_iter = 100   # maximum number of k-means iterations to preform for each initiation\n",
    "init='k-means++'    # initialization technique for kmeans, can be 'k-means++', 'random', or initial clusters to use of shape (k, n_tau_bins * n_pressure_bins)\n",
    "n_init = 10   # number of initiations of the k-means algorithm. The final result will be the initiation with the lowest calculated inertia\n",
    "gpu = False  # If the user has an Nvidia GPU, euclidean clustering can br preformed on it for a very significant speed up. CUPY/CUML must be installed in conda environment.\n",
    "\n",
    "# k sensitivity testing properties\n",
    "k_range = [2,5] # minimum and maximum values for k to test\n",
    "\n",
    "# Plot the CR centers and rfo maps? or just the correlation matricies\n",
    "plot_cr_centers = True\n",
    "plot_rfo_graphs = True\n",
    "\n",
    "# Choose whether to use a euclidean or wasserstein distance kmeans algorithm\n",
    "wasserstein_or_euclidean = \"euclidean\"\n",
    "\n",
    "# Minimum and Maximum longitudes and latitudes entered as list, or None for entire range\n",
    "lat_range = [-90,90]\n",
    "lon_range = [-180,180]\n",
    "\n",
    "# Time Range min and max, or None for all time, entered as list of str: Ex. [\"2003-03-01\", \"2004-07-01\"] or ['2003','2007']\n",
    "time_range = None\n",
    "\n",
    "# Use data only over land or over ocean\n",
    "# Set to 'L' for land only, 'O' for ocean only, or False for both land and ocean\n",
    "only_ocean_or_land = False\n",
    "# Does this dataset have a built in variable for land fraction? if so enter as a string, otherwise cartopy will be used to mask out land or water\n",
    "land_frac_var_name = None\n",
    "\n",
    "# Logging level, set to \"INFO\" for information about what the code is doing, otherwise keep at \"WARNING\"\n",
    "logging_level = 'INFO'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Openining data and preprocessing for clustering."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setting up logger\n",
    "lgr.root.setLevel(logging_level)\n",
    "# Avoid creation of large chunks with dask\n",
    "dask.config.set({\"array.slicing.split_large_chunks\": False})\n",
    "# Automatically setting premade_cloud_regimes to none because this file does not need them. Do not Change.\n",
    "premade_cloud_regimes = None\n",
    "# Setting k to an arbitrary number as it wont be used but still must be passed\n",
    "k=-1\n",
    "# Concatenating save_path and save prefix\n",
    "if save_path != None: save_path = save_path + save_prefix\n",
    "\n",
    "# Opening and preprocessing data\n",
    "mat, valid_indicies, ds, histograms, weights = open_and_process(data_path, k, tol, max_iter, init, n_init, var_name, tau_var_name, ht_var_name, lat_var_name, lon_var_name, height_or_pressure, wasserstein_or_euclidean, premade_cloud_regimes, lat_range, lon_range, time_range, only_ocean_or_land, land_frac_var_name, cluster = False)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preforming Clustering\n",
    "Preforming clustering for k in k_range and creating relevant plots. We'll debrief thes results below. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preform clustering with specified distance metric, for all values of k in k_range and create diagnostic plots\n",
    "cl_list = [] # cariable to save cl for various values of k \n",
    "ratio_list = []\n",
    "for i, k in enumerate(np.arange(k_range[0], k_range[1]+1)):\n",
    "    if wasserstein_or_euclidean == \"wasserstein\":\n",
    "        cl, cluster_labels_temp, il, cl_list = emd_means(mat, k=k, tol=tol, init=init, n_init = n_init, hard_stop=max_iter, weights=weights)\n",
    "    elif wasserstein_or_euclidean == \"euclidean\":\n",
    "        cl, cluster_labels_temp = euclidean_kmeans(k, init, n_init, mat, max_iter, tol, gpu)\n",
    "    else: raise Exception ('Invalid option for wasserstein_or_euclidean. Please enter \"wasserstein\", \"euclidean\"')\n",
    "    cl_list.append(cl)\n",
    "\n",
    "    # Reshaping cluster_labels_temp to original shape of ds and reinserting NaNs in the original places, so spatial correlations can be calcualated\n",
    "    cluster_labels = np.full(len(histograms), np.nan, dtype=np.int32)\n",
    "    cluster_labels[valid_indicies]=cluster_labels_temp\n",
    "    cluster_labels = xr.DataArray(data=cluster_labels, coords={\"spacetime\":histograms.spacetime},dims=(\"spacetime\") )\n",
    "    cluster_labels = cluster_labels.unstack()\n",
    "\n",
    "    # Plotting correlation matrices\n",
    "    histogram_cor(cl, save_path)\n",
    "    spacial_cor(cluster_labels_temp,k, save_path)\n",
    "    \n",
    "    if i >0: kp1_histogram_cor(cl_list[i], cl_list[i-1], save_path)\n",
    "\n",
    "    if plot_cr_centers:\n",
    "        plot_hists(cluster_labels, k, ds, ht_var_name, tau_var_name, valid_indicies, mat, cluster_labels_temp, height_or_pressure, save_path)\n",
    "\n",
    "    if plot_rfo_graphs:\n",
    "        plot_rfo(cluster_labels,k,ds, save_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the ISCCP toy data included in this cookbook, which includes only two years of monthly data from 30N to 30S, we see at k=2 it is initially seperated into essentially a low cloud and high cloud CR. Increasing k to 3 breaks up the low cloud CR into a thicker stratocumulus regime, and a thinner more general shollow convection regime. Increasing once again to K=4 breaks up the high cloud CR into a deep convection and a more general high cloud/cirrus CR. At k=5 our general low cloud/shallow convection CR splits once again, however this time, the split is highly correlated with other cloud regimes created in k=5. So here, I'd say that k=4 is the most ideal. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What's next?\n",
    "Next we'll test how robust our k-means setup is, and make sure we can repeat the results of this clustering. To do this, select the file called cluster_robustness_testing.ipynb."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('clustering': virtualenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false,
  "interpreter": {
   "hash": "5c82514a3a5bab9dc8fa7ef0461220e9cb3d38a64f7d511dfc68fe93fbd887e3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}