{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Plot and Analyze "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "In this document we'll take our premade cloud regimes and create plots of the cluster centers and spatial distributions. You'll also learn how to extract data about a specific cloud regime. Yes, we've already made plenty of plots of cluster centers and spatial distributions, however this script does it differently. This script preforms NO CLUSTERING. It only takes a set of premade cloud regimes and then fits the data it is passed into them. This is MUCH faster than preforming the clustering again and allows one to apply cloud regimes to sperate data. For example, we created CRs from ISCCP observation data in this tutorial. Using this script, you could apply those CRs to data produced by models running with ISCCP satellite simulator and examine them. You’d do this by setting data_path as a path to the model data, but still passing the ISCCP observation cloud regimes to the premade_cloud_regimes variable. Also, at the end of the script you'll learn how to use the outputs to further explore any other variables of interest in relation to your cloud regimes. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Intermediate NumPy](https://foundations.projectpythia.org/core/numpy/intermediate-numpy.html) | Necessary | |\n",
    "| [Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Necessary | |\n",
    "| [Understanding of NetCDF](https://foundations.projectpythia.org/core/data-formats/netcdf-cf.html) | Helpful | Familiarity with metadata structure |\n",
    "| [Introduction to Pandas](https://foundations.projectpythia.org/core/pandas/pandas.html) | Helpful | Familiarity with labeled data |\n",
    "\n",
    "- **Time to learn**: 15 minutes\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from Functions import plot_hists, plot_rfo, open_and_process\n",
    "import logging as lgr\n",
    "import xarray as xr\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we define the variables necessary to begin using the toy ISCCP dataset included with this cookbook. To start off leave these variables alone, but later on feel free to experiment. \n",
    "\n",
    "If running locally with your own dataset, you will need to change many of variables these to match and point towards your data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Path to data to cluster\n",
    "data_path = \"./ISCCP_toy_data.nc\"\n",
    "\n",
    "# Path to the directory you wish to save plots in if running as a script, if None plots will only be shown and not saved. Enter as String\n",
    "save_path = None\n",
    "save_prefix = None  # prefix to put into the name of each plot to distinguish different runs of this script\n",
    "\n",
    "# Variable name of data to cluster in data_path\n",
    "# Name of tau dimension for var_name\n",
    "# Name of height/pressure dimension for var_name\n",
    "var_name =  'n_pctaudist' \n",
    "tau_var_name =  'levtau' \n",
    "ht_var_name =  'levpc'\n",
    "lat_var_name = 'lat'\n",
    "lon_var_name = 'lon'\n",
    "\n",
    "# Does this dataset use cloud top height or cloud top pressure? enter \"h\" for height or \"p\" for pressure\n",
    "height_or_pressure = 'p'\n",
    "\n",
    "# The premade cloud regimes to use. The script will then fit the histograms located in the files in data_path into these CRs.\n",
    "# Set this variable to a path to a numpy ndarray, or an xarray data array of premade cloud regimes of shape=(k, n_tau_bins * n_pressure_bins)\n",
    "premade_cloud_regimes = \"./toy_ISCCP_cluster_centers.npy\"\n",
    "\n",
    "# Choose whether to use a euclidean or wasserstein distance\n",
    "wasserstein_or_euclidean = \"euclidean\"\n",
    "\n",
    "# Minimum and Maximum longitudes and latitudes entered as list, or None for entire range\n",
    "lat_range = [-90,90]\n",
    "lon_range = [-180,180]\n",
    "\n",
    "# Time Range min and max, or None for all time, entered as list of str: Ex. [\"2003-03-01\", \"2004-07-01\"] or ['2003','2007']\n",
    "time_range = None\n",
    "\n",
    "# Use data only over land or over ocean\n",
    "# Set to 'L' for land only, 'O' for ocean only, or False for both land and ocean\n",
    "only_ocean_or_land = False\n",
    "# Does this dataset have a built in variable for land fraction? if so enter as a string, otherwise cartopy will be used to mask out land or water\n",
    "land_frac_var_name = None\n",
    "\n",
    "# Logging level, set to \"INFO\" for information about what the code is doing, otherwise keep at \"WARNING\"\n",
    "logging_level = 'INFO'\n",
    "\n",
    "# Setting up logger\n",
    "lgr.root.setLevel(logging_level)\n",
    "# Concatenating save_path and save prefix\n",
    "if save_path != None: save_path = save_path + save_prefix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Opening, Labeling and Plotting\n",
    "Here we open our data and assign each histogram to the nearest cluster. We store this information in an array named cluster_labels. After this we call plot_hists() to create plots of the CR centers, then we call plot_rfo() to create plots of the spatial distribution of each CR."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Automatically setting k from the premade cloud regimes\n",
    "k = len(np.load(premade_cloud_regimes))\n",
    "# Setting k-means properteis to None since they arent used but still must be passed\n",
    "tol, max_iter, init, n_init, gpu = None, None, None, None, None\n",
    "# Opening data, and clustering\n",
    "mat, cluster_labels, cluster_labels_temp, valid_indicies, ds = open_and_process(data_path, k, tol, max_iter, init, n_init, var_name, tau_var_name, ht_var_name, lat_var_name, lon_var_name, height_or_pressure, wasserstein_or_euclidean, premade_cloud_regimes, lat_range, lon_range, time_range, only_ocean_or_land, land_frac_var_name, cluster=True, gpu=gpu)\n",
    "# Plotting histograms\n",
    "plot_hists(cluster_labels, k, ds, ht_var_name, tau_var_name, valid_indicies, mat, cluster_labels_temp, height_or_pressure, save_path)\n",
    "# Plotting RFO\n",
    "plot_rfo(cluster_labels, k ,ds, save_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further Analysis\n",
    "This is all well and good, but this analysis only goes so far. It's likely a user will want to do further analysis beyond looking at the spatial distributions of the CRs. In this section you will learn how to do this. \n",
    "\n",
    "Described above, we create an array called cluster_labels that holds the cluster that each histogram belongs to. Here's how to use it:\n",
    "\n",
    "First lets re-open the dataset specified by the data_path variable, since all the variables other than the one stored in var_name are automatically removed. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = xr.open_mfdataset(data_path)\n",
    "ds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The optical depth - cloud top height joint histograms should be stored in ds under the variable entered as var_name, such that ds[var_name] opens the raw histogram data. For ISCCP, this data is in the shape (n_time, n_tau, n_pressures, n_lat, n_lon) where n_time, n_tau, n_pressures... are the number of times, tau bins, pressure bins etc that are present in the dataarray. Other data products will be roughly similar to this but may have the dimensions in a different order. cluster_labels is an array of integers in (almost) this same shape, but without n_tau and n_pressure dimensions. So, in this case cluster labels is shape (n_time, n_lat, n_lon). The value of cluster_labels[specific_time, specific_lat, specific_lon] will be an integer ranging from 0 to k-1 that indicates which CR the histogram located at ds[var_name][specific_time,:,:, specific_lat, specific_lon] belongs to. This may be slightly confusing, but a value of 0 indicates this histogram belongs to CR1, a value of 1 indicates it belongs to CR2, 2 to CR3 etc... This indexing works better for some operations in python, but if it truly displeases you, there is a very easy fix. After all the pre-written function calls but before starting your analysis, simply add the line \"cluster_labels += 1\". This will reindex the values to run from 1 through k, where each value is directly equal to the CR the corresponding histogram has been labeled as. It is also important to note that any histogram that contains a NaN will not be assigned a cluster, and its corresponding value in cluster labels will also be NaN.\n",
    "\n",
    "Now that we understand the structure of the cluster_labels array, let’s go over how to use it. As can be seen above, our ds has two variables: n_pctaudist which holds the histogram data, and pc which holds the mean cloud-top pressure for cloudy pixels in each histogram. We will use the cluster labels array and the pc variable to calculate the mean cloud-top pressure for each CR. (Yes, this could also be calculated from the raw histogram data, but here we use a separate variable to illustrate how to do this for a variable that can’t simply be derived from the histogram data.)\n",
    "\n",
    "Now, let’s create a dataarray of the mean cloud-top pressures."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pc = ds.pc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can loop through each CR and calculate the average cloud top pressure. We do this with the dataarray.where() function provided by xarray. This function (used on line 2 in the following cell) allows us to grab all the values of pc where cluster_labels is equal to a certain cluster and mask out all the other values. We can then take the mean of this and get the mean cloud top pressure for a certain CR. \n",
    "\n",
    "This method can be extended to be used on any variable, not just average cloud top pressure. It is also possible to remove the for loop in the below cell and preform this operation using only broadcasting and vectorization if one wishes. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Looping through each cluster\n",
    "for cr in range(k):\n",
    "    # Grabbing the values of pc for histograms assigned to cr\n",
    "    pc_of_cr = pc.where(cluster_labels == cr)\n",
    "    # Calculating the mean cloud top pressure\n",
    "    average_cloud_top_pressure = np.nanmean(pc_of_cr)\n",
    "    # Printing results\n",
    "    print(f\"The average cloud top pressure of CR{cr+1} is {average_cloud_top_pressure} \")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "You've now learned to create and analyze cloud regimes! This process can be used on any data product that contains joint cloud top height and optical depth histograms, not just ISCCP. So go out there and get clustering on your own data!"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('clustering': virtualenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false,
  "interpreter": {
   "hash": "5c82514a3a5bab9dc8fa7ef0461220e9cb3d38a64f7d511dfc68fe93fbd887e3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}